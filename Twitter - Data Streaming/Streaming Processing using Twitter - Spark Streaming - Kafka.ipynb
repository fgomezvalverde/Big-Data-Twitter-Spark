{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Processing using Twitter - Spark Streaming - Kafka\n",
    "\n",
    "This project is for begginer in the Big Data , you will find a very simple project for testing porpuses.\n",
    "Let looks what we are going to talk about:\n",
    "\n",
    "1. Objective of the project.\n",
    "2. Architecture\n",
    "3. Sofware Installation\n",
    "4. Implementation\n",
    "5. Results\n",
    "6. Conclusions\n",
    "\n",
    "All the source code is in the next URL: https://github.com/fgomezvalverde/BigDataCourse/tree/master/Twitter%20-%20Data%20Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Objective of the project\n",
    "Create a streaming processing data stack using Twitter as a datasource. Get know of the use of tecnologies like: Kafka, Zookeeper, Streaming, Brockers and others. And have a small view of what can be done using this tecnologies in real life scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture\n",
    "<img src=\"Diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Sofware Installation\n",
    "\n",
    "1. Installing Zookepeer\n",
    "https://medium.com/@shaaslam/installing-apache-zookeeper-on-windows-45eda303e835\n",
    "\n",
    "This a beautiful explanation of what is Zookepeer for Kafka: https://www.cloudkarafka.com/blog/2018-07-04-cloudkarafka_what_is_zookeeper.html\n",
    "\n",
    "2. Installing Kafka\n",
    "https://medium.com/@shaaslam/installing-apache-kafka-on-windows-495f6f2fd3c8\n",
    "\n",
    "3. Test Zooker and kafka. Making a producer and consumer test from Command Line.\n",
    "https://medium.com/@shaaslam/installing-apache-kafka-on-windows-495f6f2fd3c8 ***This one is an exellente post, just have one issue the consumer should listen to the broker, and not to zookeeper: localhost:9092***\n",
    "\n",
    "Theres other considerations that you need to have on mind, like; installed JRE, 7-ZIP for tar unziping, Python, Conda with PySpark, the spark streaming kafka assembly. Theres is lots of resources out there to do this things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Implementation\n",
    "\n",
    "\n",
    "Afther the installation of Zookepeer and Kafka, and did a very simple example of the Producer - Consumer in the console. We are ready to start for a more real case scenario, like Twitter as Producer and a Spark Streaming Context as Consumer. There is two approches for the Spark Kafka Streaming, for this example we will use Reciever-based. For more information you can follow this linK: https://spark.apache.org/docs/latest/streaming-kafka-0-8-integration.html\n",
    "\n",
    "Another thing, I created a topic called **raw-twitter** and parting from that you already have this. You did this already in the Producer - Consumer test from Command with **sql-insert** topic.\n",
    "\n",
    "### 4.1.Producer\n",
    "\n",
    "The consumer contains 6 Subject:\n",
    "- Imports\n",
    "- Configuration\n",
    "- Authentication Method\n",
    "- Get Tweets URL Constructions\n",
    "- Publish Message to Kafka\n",
    "- KeepAlive Process\n",
    "\n",
    "We will focus on the Configuration and the Publish Message to Kafka sections. Still take a look to the others methods in https://github.com/fgomezvalverde/BigDataCourse/blob/master/Twitter%20-%20Data%20Streaming/Producer.ipynb.\n",
    "\n",
    "\n",
    "#### Configuration\n",
    "```\n",
    "consumerKey=\"XXXXXXXXXXXXXXXXXX\"\n",
    "consumerSecret=\"XXXXXXXXXXXXXXXXXX\"\n",
    "accessToken=\"XXXXXXXXXXXXXXXXXX\"\n",
    "accessTokenSecret=\"XXXXXXXXXXXXXXXXXX\"\n",
    "keyword= \"superbowl\"\n",
    "start_date = \"2020-01-01\" \n",
    "end_date = \"2020-03-03\"\n",
    "req_count = 0\n",
    "min_faves=60000\n",
    "change=10000 \n",
    "interval = 500\n",
    "lang=\"en\"\n",
    "\n",
    "kafka_url=\"localhost:9092\"\n",
    "topic_name= \"raw-twitter\"\n",
    "```\n",
    "**The Twitter configuration** is mostly API KEYS, the other part is the hashtag that be looking \"superbowl\", in the specific dates with a very high Faves on it. Soo would find posts like from Shakira. \n",
    "\n",
    "For **kafka_url**, Im using the same computer for my kafka server and the producer soo localhost is the right for me , but is you are using another computer or a cloud server you need to change this variable.\n",
    "\n",
    "And **topic_name**, like I said before Im using **raw-twitter** as topic, you need to changed this to your created topic on kafka.\n",
    "\n",
    "#### Publish Message to Kafka\n",
    "\n",
    "\n",
    "```\n",
    "def connect_kafka_producer():\n",
    "    producer = None\n",
    "    try:\n",
    "        producer = KafkaProducer(bootstrap_servers=[kafka_url], api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return producer\n",
    "```\n",
    "Using the kafka_url we create the Kafka Producer instance , very basic by the way. Is possible that theres more configuration variables . Take a look the KakfaProducer API.\n",
    "\n",
    "```\n",
    "def publish_message(producer_instance, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        print('Message published successfully.')\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message')\n",
    "        print(str(ex))\n",
    "```\n",
    "This one is in charge of the publish of the message using the Kafka Producer and the **Topic name** setter on the configuration section.\n",
    "\n",
    "\n",
    "### 4.2.Consumer\n",
    "\n",
    "The consumer contains 3 Subject:\n",
    "- Imports\n",
    "- Configuration\n",
    "- KeepAlive Process\n",
    "\n",
    "We will focus on the Configuration and the KeepAlive Process sections. Still take a look to the others methods in https://github.com/fgomezvalverde/BigDataCourse/blob/master/Twitter%20-%20Data%20Streaming/Consumer.ipynb.\n",
    "\n",
    "#### Configuration\n",
    "\n",
    "```\n",
    "zookeeper_url=\"localhost:2181\"\n",
    "topic_name= \"raw-twitter\"\n",
    "```\n",
    "The consumer need to point to the Zookepeer URL. In my case ***localhost:2181*** . And the topic name is the same that we define before ***raw-twitter***.\n",
    "\n",
    "```\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars spark-streaming-kafka-0-8-assembly_2.11-2.4.4.jar pyspark-shell' \n",
    "sc = SparkContext(appName=\"PythonStreamingRecieverKafkaTwitter\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, 10) # 10 second window\n",
    "kvs = KafkaUtils.createStream(ssc,zookeeper_url,\"raw\",{topic_name:1}) \n",
    "lines = kvs.map(lambda x: x[1])\n",
    "lines.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination(30)\n",
    "ssc.stop()\n",
    "```\n",
    "\n",
    "For this part I recommend to this post first, is a very good explanation about the approch https://spark.apache.org/docs/1.4.1/streaming-kafka-integration.html. And also is important to know what is a DStream and how it works, I recommend to read this : https://stackoverflow.com/questions/36421619/whats-the-meaning-of-dstream-foreachrdd-function. \n",
    "\n",
    "Basicly we read every 10 seconds ***StreamingContext(sc, 10)***  any message from the topic  one by one ***{topic_name:1}***.\n",
    "\n",
    "And finally print every message until an error or interruption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Demostration\n",
    "\n",
    "Take a look of the video of a test using the same computer the Producer and Consumer.\n",
    "\n",
    "You can download the video here: https://github.com/fgomezvalverde/BigDataCourse/raw/master/Twitter%20-%20Data%20Streaming/Localhost%20Demo.mp4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Conclusions\n",
    "\n",
    "All the configuration set here was for tests purposes. Using the same computer for Producer and Consumer is not a Production scenario. Also, we work with just 1 Brocker for the Kafka Server, depends of the volumen of the data, replications, disponibility, etc. We can found a better configutation dependes on the sceneario.\n",
    "\n",
    "What steps can be next from here:\n",
    "- Using a diferents computer for the Kafka Server, Producer and Consumer.\n",
    "- Taking more data and use more brockers.\n",
    "- Analize the twitters messages with sentiment analisys and maybe use it as a Y parameter for a machiner learning project.\n",
    "- Save the data on a data base, using this stream Processing and maybe can see what to do with batch processing. Databases like Hadoop or Cassandra can be interesenting to integrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

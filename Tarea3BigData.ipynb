{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Tarea #3 consiste en instalar PostgreSQL, para luego ejecutar el código en la notebook ubicada en https://github.com/fmezacr/BigDataTEC/tree/master/Clase1/DB  y hacer el ejercicio indicado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la tarea asumiremos que una tienda en línea almacena las compras de sus clientes en una base de datos PostgreSQL. Aplicaremos ciertas operaciones básicas sobre los datos para mostrar lo que este tipo de plataformas ofrecen. A continuación, un ejemplo básico de modelo de datos, que se usará posteriormente."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE TABLE transactions (\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  customer_id integer NOT NULL,\n",
    "  amount integer NOT NULL,\n",
    "  purchased_at timestamp without time zone NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "INSERT INTO \"transactions\" (customer_id, amount, purchased_at) VALUES\n",
    "(1, 55, '2017-03-01 09:00:00'),\n",
    "(1, 125, '2017-03-01 10:00:00'),\n",
    "(1, 32, '2017-03-02 13:00:00'),\n",
    "(1, 64, '2017-03-02 15:00:00'),\n",
    "(1, 128, '2017-03-03 10:00:00'),\n",
    "(2, 333, '2017-03-01 09:00:00'),\n",
    "(2, 334, '2017-03-01 09:01:00'),\n",
    "(2, 333, '2017-03-01 09:02:00'),\n",
    "(2, 11, '2017-03-03 20:00:00'),\n",
    "(2, 44, '2017-03-03 20:15:00');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente ejemplo de código utiliza la base de datos descrita anteriormente y muestra el contenido almacenado en ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark/spark')\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, udf \n",
    "from pyspark.sql.types import DateType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Basic JDBC pipeline\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/postgresql-42.2.9.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/postgresql-42.2.9.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+\n",
      "| id|customer_id|amount|       purchased_at|\n",
      "+---+-----------+------+-------------------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00|\n",
      "|  2|          1|   125|2017-03-01 10:00:00|\n",
      "|  3|          1|    32|2017-03-02 13:00:00|\n",
      "|  4|          1|    64|2017-03-02 15:00:00|\n",
      "|  5|          1|   128|2017-03-03 10:00:00|\n",
      "|  6|          2|   333|2017-03-01 09:00:00|\n",
      "|  7|          2|   334|2017-03-01 09:01:00|\n",
      "|  8|          2|   333|2017-03-01 09:02:00|\n",
      "|  9|          2|    11|2017-03-03 20:00:00|\n",
      "| 10|          2|    44|2017-03-03 20:15:00|\n",
      "+---+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading single DataFrame in Spark by retrieving all rows from a DB table.\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost/postgres\") \\\n",
    "    .option(\"user\", \"caro\") \\\n",
    "    .option(\"password\", \"Caro2017\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"transactions\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza date_format en el módulo pyspark.sql.functions. El siguiente código crea una columna nueva después de aplicar la transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+-----------+\n",
      "| id|customer_id|amount|       purchased_at|date_string|\n",
      "+---+-----------+------+-------------------+-----------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00| 03/01/2017|\n",
      "|  2|          1|   125|2017-03-01 10:00:00| 03/01/2017|\n",
      "|  3|          1|    32|2017-03-02 13:00:00| 03/02/2017|\n",
      "|  4|          1|    64|2017-03-02 15:00:00| 03/02/2017|\n",
      "|  5|          1|   128|2017-03-03 10:00:00| 03/03/2017|\n",
      "|  6|          2|   333|2017-03-01 09:00:00| 03/01/2017|\n",
      "|  7|          2|   334|2017-03-01 09:01:00| 03/01/2017|\n",
      "|  8|          2|   333|2017-03-01 09:02:00| 03/01/2017|\n",
      "|  9|          2|    11|2017-03-03 20:00:00| 03/03/2017|\n",
      "| 10|          2|    44|2017-03-03 20:15:00| 03/03/2017|\n",
      "+---+-----------+------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "formatted_df = df.withColumn(\"date_string\", date_format(col(\"purchased_at\"), 'MM/dd/yyyy'))\n",
    "formatted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una manera cómoda de encapsular funciones de Python que deben ser aplicadas a celdas del Dataframe, pero aún no tienen su implementación en las bibliotecas.\n",
    "\n",
    "El siguiente ejemplo muestra la columna creada previamente (tipo string) y la transforma en un tipo DateType propio de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "| id|customer_id|amount|       purchased_at|date_string|      date|\n",
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
      "|  2|          1|   125|2017-03-01 10:00:00| 03/01/2017|2017-03-01|\n",
      "|  3|          1|    32|2017-03-02 13:00:00| 03/02/2017|2017-03-02|\n",
      "|  4|          1|    64|2017-03-02 15:00:00| 03/02/2017|2017-03-02|\n",
      "|  5|          1|   128|2017-03-03 10:00:00| 03/03/2017|2017-03-03|\n",
      "|  6|          2|   333|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
      "|  7|          2|   334|2017-03-01 09:01:00| 03/01/2017|2017-03-01|\n",
      "|  8|          2|   333|2017-03-01 09:02:00| 03/01/2017|2017-03-01|\n",
      "|  9|          2|    11|2017-03-03 20:00:00| 03/03/2017|2017-03-03|\n",
      "| 10|          2|    44|2017-03-03 20:15:00| 03/03/2017|2017-03-03|\n",
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- purchased_at: timestamp (nullable = true)\n",
      " |-- date_string: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_to_date = \\\n",
    "    udf(lambda text_date: datetime.strptime(text_date, '%m/%d/%Y'),\n",
    "        DateType())\n",
    "\n",
    "typed_df = formatted_df.withColumn(\"date\", string_to_date(formatted_df.date_string))\n",
    "typed_df.show()\n",
    "typed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para sumar los datos podemos utilizar el concepto común de agrupamiento en SQL. Spark posee una agrupación de groupBy, directamente. En este caso, queremos sumar todas las compras por cliente y día:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+----------------+-----------+\n",
      "|customer_id|      date|sum(id)|sum(customer_id)|sum(amount)|\n",
      "+-----------+----------+-------+----------------+-----------+\n",
      "|          2|2017-03-01|     21|               6|       1000|\n",
      "|          1|2017-03-02|      7|               2|         96|\n",
      "|          1|2017-03-03|      5|               1|        128|\n",
      "|          1|2017-03-01|      3|               2|        180|\n",
      "|          2|2017-03-03|     19|               4|         55|\n",
      "+-----------+----------+-------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_df = typed_df.groupBy(\"customer_id\", \"date\").sum()\n",
    "sum_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible dar a las columnas un nombre más amigable con los consumidores de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|          2|2017-03-01|  1000|\n",
      "|          1|2017-03-02|    96|\n",
      "|          1|2017-03-03|   128|\n",
      "|          1|2017-03-01|   180|\n",
      "|          2|2017-03-03|    55|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats_df = \\\n",
    "    sum_df.select(\n",
    "        col('customer_id'),\n",
    "        col('date'),\n",
    "        col('sum(amount)').alias('amount'))\n",
    "\n",
    "stats_df.printSchema()\n",
    "stats_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra como cargar datos de un archivo CSV que contiene los nombres de los clientes, así como la moneda en que realizan transacciones. Nótese que el CSV no tiene información de tipos de datos, por lo que es buena práctica agregarlos explícitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+---+----+--------+\n",
      "| id|name|currency|\n",
      "+---+----+--------+\n",
      "|  1|John|     CRC|\n",
      "|  2|Jane|     EUR|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "names_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"names.csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(StructType([\n",
    "                StructField(\"id\", IntegerType()),\n",
    "                StructField(\"name\", StringType()),\n",
    "                StructField(\"currency\", StringType())])) \\\n",
    "    .load()\n",
    "\n",
    "names_df.printSchema()\n",
    "names_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que la información fue cargada, la fuente particular no es relevante. A continuación se muestra como podemos enriquerecer la información utilizando la funcion JOIN entre data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+----------+------+---+----+--------+\n",
      "|customer_id|      date|amount| id|name|currency|\n",
      "+-----------+----------+------+---+----+--------+\n",
      "|          2|2017-03-01|  1000|  2|Jane|     EUR|\n",
      "|          1|2017-03-02|    96|  1|John|     CRC|\n",
      "|          1|2017-03-03|   128|  1|John|     CRC|\n",
      "|          1|2017-03-01|   180|  1|John|     CRC|\n",
      "|          2|2017-03-03|    55|  2|Jane|     EUR|\n",
      "+-----------+----------+------+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joint_df = stats_df.join(names_df, stats_df.customer_id == names_df.id)\n",
    "joint_df.printSchema()\n",
    "joint_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
